#------------------------------------------------------------------------------------------------------
# Script-file:   WAUS2019.R
# Author:        Chun Wen Khoo
#	Project:       Rain in Australia
# Purpose:  	   Predict rain tomorrow in Australia
#------------------------------------------------------------------------------------------------------
# Clean working environment
rm(list = ls())
# Set number of significant digits to 4
options(digits = 4)
#------------------------------------------------------------------------------------------------------
# Loading data, sample 2000 rows and 10 locations
#------------------------------------------------------------------------------------------------------
WAUS <- read.csv("WAUS2019.csv")
L <- as.data.frame(c(1:49))
set.seed(12345678)
L <- L[sample(nrow(L), 10, replace = FALSE),] # sample 10 locations
WAUS <- WAUS[(WAUS$Location %in% L),]
WAUS <- WAUS[sample(nrow(WAUS), 2000, replace = FALSE),] # sample 2000 rows
#------------------------------------------------------------------------------------------------------
# Data pre-processing
#------------------------------------------------------------------------------------------------------
# Plot of proportion of rainy days to fine days
library(reshape2)
library(ggplot2)
w <- WAUS[c('Location', 'RainToday', 'RainTomorrow')]
w <- w[!(is.na(w$RainToday)) & !(is.na(w$RainTomorrow)),]
dfm <- melt(w, id.vars = 'Location')
View(dfm)
g <- ggplot(dfm, aes(x = variable, fill = value)) + geom_bar(position = "dodge") + ggtitle("Proportion of rainy days and fine days")
g <- g + labs(y = "Number of days")
g
# Proportion of actual rainy days to actual fine days
cur <- WAUS[!(is.na(WAUS$RainToday)),]
rain <- cur[cur$RainToday == "Yes",]
fine <- cur[cur$RainToday == "No",]
nrow(fine)/nrow(rain)
# Proportion of predicted rainy days to predicted fine days
cur <- WAUS[!(is.na(WAUS$RainTomorrow)),]
rain <- cur[cur$RainTomorrow == "Yes",]
fine <- cur[cur$RainTomorrow == "No",]
nrow(fine)/nrow(rain)
# Summary of each numeric predictor
library(pastecs)
stat.desc(WAUS[c(5:9,11,14:23)], norm = FALSE)
#------------------------------------------------------------------------------------------------------
# Attributes to omit from analysis:
# Day, month, and year :- would not contribute to the prediction of weather of the next day.
# Location :- Only want to predict if it will rain in Australia, hence not location-specific.
# Evaporation, sunshine, Cloud9am, Cloud3pm :- ignore columns with less data (threshold ~ 60%)
colSums(!is.na(WAUS))/2000*100
WAUS <- WAUS[,c(5:7,10:19,22:25)]
# Remove rows that contain NAs
WAUS <- na.omit(WAUS)
# Split to training and testing sets
set.seed(12345678)
train.row = sample(1:nrow(WAUS), 0.7*nrow(WAUS))
WAUS.train = WAUS[train.row,]
WAUS.test = WAUS[-train.row,]
#------------------------------------------------------------------------------------------------------
# Libraries to be used
library(tree)
library(e1071)
library(adabag)
library(randomForest)
library(ROCR)
#------------------------------------------------------------------------------------------------------
# Classification models
#------------------------------------------------------------------------------------------------------
# Decision tree
# 1. Fitting the model
set.seed(12345678)
WAUS.tree <- tree(RainTomorrow~., data = WAUS.train)
plot(WAUS.tree)
text(WAUS.tree, pretty = 0)
# 2. Predict using test set, produce confusion matrix
tpred <- predict(WAUS.tree, WAUS.test, type = "class")
tpred <- table(actual = WAUS.test$RainTomorrow, predicted = tpred)
t.acc <- (tpred[1] + tpred[4])/sum(tpred)
t.acc
# 3. Calculate confidence of predicting 'Yes' for each case and plot ROC curve for each classifier
tpred.r <- predict(WAUS.tree, WAUS.test, type = "vector")
tpred <- prediction(tpred.r[,2], WAUS.test$RainTomorrow)
tperf <- performance(tpred, "tpr", "fpr")
auc_tree <- performance(tpred, measure = "auc")
#------------------------------------------------------------------------------------------------------
# Naive Bayes
# 1. Fitting the model
set.seed(12345678)
WAUS.nbmodel <- naiveBayes(RainTomorrow~., data = WAUS.train)
# 2. Predict using test set, produce confusion matrix
nbpred <- predict(WAUS.nbmodel, WAUS.test)
nbpred <- table(actual = WAUS.test$RainTomorrow, predicted = nbpred)
nb.acc <- (nbpred[1] + nbpred[4])/sum(nbpred)
nb.acc
# 3. Calculate confidence of predicting 'Yes' for each case and plot ROC curve for each classifier
nbpred.r <- predict(WAUS.nbmodel, WAUS.test, type = "raw")
nbpred <- prediction(nbpred.r[,2], WAUS.test$RainTomorrow)
nbperf <- performance(nbpred, "tpr", "fpr")
auc_nb <- performance(nbpred, measure = "auc")
#------------------------------------------------------------------------------------------------------
# Bagging
# 1. Fitting the model
set.seed(12345678)
WAUS.bag <- bagging(RainTomorrow~., data = WAUS.train, mfinal = 10)
# 2. Predict using test set, produce confusion matrix
ibpred <- predict.bagging(WAUS.bag, newdata = WAUS.test)
ibpred$confusion
ib.acc <- (ibpred$confusion[1] + ibpred$confusion[4])/sum(ibpred$confusion)
ib.acc
# 3. Calculate confidence of predicting 'Yes' for each case and plot ROC curve for each classifier
ibpred <- predict.bagging(WAUS.bag, newdata = WAUS.test)
ibpred <- prediction(ibpred$prob[,2], WAUS.test$RainTomorrow)
ibperf <- performance(ibpred, "tpr", "fpr")
auc_ib <- performance(ibpred, measure = "auc")
#------------------------------------------------------------------------------------------------------
# Boosting
# 1. Fitting the model
set.seed(12345678)
WAUS.boost <- boosting(RainTomorrow~., data = WAUS.train, mfinal = 10)
# 2. Predict using test set, produce confusion matrix
ibopred <- predict.boosting(WAUS.boost, newdata = WAUS.test)
ibopred <- table(observed = WAUS.test$RainTomorrow, predicted = ibopred$class)
ibo.acc <- (ibopred[1]+ibopred[4])/sum(ibopred)
ibo.acc
# 3. Calculate confidence of predicting 'Yes' for each case and plot ROC curve for each classifier
ibopred.r <- predict(WAUS.boost, WAUS.test, type = "raw")
ibopred <- prediction(ibopred.r$prob[,2], WAUS.test$RainTomorrow)
iboperf <- performance(ibopred, "tpr", "fpr")
auc_ibo <- performance(ibopred, measure = "auc")
#------------------------------------------------------------------------------------------------------
# Random forest
# 1. Fitting the model
set.seed(12345678)
WAUS.rf <- randomForest(RainTomorrow~., data = WAUS.train, importance = TRUE)
# 2. Predict using test set, produce confusion matrix
rfpred <- predict(WAUS.rf, WAUS.test)
rfpred <- table(observed = WAUS.test$RainTomorrow, predicted = rfpred)
rf.acc <- (rfpred[1] + rfpred[4])/sum(rfpred)
rf.acc
# 3. Calculate confidence of predicting 'Yes' for each case and plot ROC curve for each classifier
rfpred.r <- predict(WAUS.rf, WAUS.test, type = "prob")
rfpred <- prediction(rfpred.r[,2], WAUS.test$RainTomorrow)
rfperf <- performance(rfpred, "tpr", "fpr")
auc_rf <- performance(rfpred, measure = "auc")
#------------------------------------------------------------------------------------------------------
# ROC curve of each classifier
plot(tperf, col = "brown")
plot(nbperf, add = TRUE, col = "blue")
plot(ibperf, add = TRUE, col = "black")
plot(iboperf, add = TRUE, col = "red")
plot(rfperf, add = TRUE, col = "green")
abline(0,1)
title("ROC of classifiers")
legend('bottomright',
c("Decision Tree", "Naive Bayes", "Bagging", "Boosting", "Random Forest"),
lty=1, col=c('brown', 'blue', 'black',' red', 'green'))
# Accuracy and AUC of each classifier
classifier <- c("Decision Tree", "Naive Bayes","Bagging", "Boosting", "Random Forest")
acc <- c(t.acc, nb.acc, ib.acc, ibo.acc, rf.acc)
auc <- c(as.numeric(auc_tree@y.values), as.numeric(auc_nb@y.values),
as.numeric(auc_ib@y.values), as.numeric(auc_ibo@y.values), as.numeric(auc_rf@y.values))
class_acc_auc <- data.frame(classifier, acc, auc)
class_acc_auc
#------------------------------------------------------------------------------------------------------
# Which variables are most important in predicting weather, and which variables can be omitted?
# Important predictors from decision tree
summary(WAUS.tree)
# Important predictors from bagging
WAUS.bag$importance
# Important predictors from boosting
WAUS.boost$importance
# Important predictors from random forest
importance(WAUS.rf)
#------------------------------------------------------------------------------------------------------
# Experiment with classifier settings (for at least of one model). Show improved ROC, AUC, or other accuracy measures.
# Report parameter settings and assumptions made
#------------------------------------------------------------------------------------------------------
# CV and pruning with decision tree
# 1. Fitting the model
set.seed(12345678)
WAUS.tree.cv <- cv.tree(WAUS.tree, FUN = prune.misclass)
print(WAUS.tree.cv)
WAUS.prune <- prune.misclass(WAUS.tree, best = 5)
plot(WAUS.prune)
text(WAUS.prune, pretty = 0)
# 2. Predict using test set
prune.tpred <- predict(WAUS.prune, WAUS.test, type = "class")
prune.tpred <- table(actual = WAUS.test$RainTomorrow, predicted = prune.tpred)
prune.t.acc <- (prune.tpred[1] + prune.tpred[4])/sum(prune.tpred)
prune.t.acc
prune.tpred.r <- predict(WAUS.prune, WAUS.test, type = "vector")
prune.tpred <- prediction(prune.tpred.r[,2], WAUS.test$RainTomorrow)
prune.tperf <- performance(prune.tpred, "tpr", "fpr")
auc_ptree <- performance(prune.tpred, measure = "auc")
# 3. Comparison with original decision tree
plot(prune.tperf, col = "blue")
plot(tperf, col = "red", add = TRUE)
abline(0,1)
legend('bottomright', c("Original", "CV with pruning"), lty=1, col=c('blue', 'red'))
print(as.numeric(auc_ptree@y.values))
dt <- data.frame(c(t.acc, prune.t.acc), c(as.numeric(auc_tree@y.values), as.numeric(auc_ptree@y.values)))
colnames(dt) <- c("Acc", "AUC")
rownames(dt) <- c("Original", "CV with pruning")
dt
#------------------------------------------------------------------------------------------------------
# Bagging with mfinal = 100
# 1. Fitting the model
set.seed(12345678)
WAUS.bag <- bagging(RainTomorrow~., data = WAUS.train, mfinal = 100)
# 2. Predict using test set, produce confusion matrix
ibpred <- predict.bagging(WAUS.bag, newdata = WAUS.test)
ibpred$confusion
ib.cv.acc <- (ibpred$confusion[1] + ibpred$confusion[4])/sum(ibpred$confusion)
ib.cv.acc
# 3. Calculate confidence of predicting 'Yes' for each case and plot ROC curve for each classifier
ibpred <- predict.bagging(WAUS.bag, newdata = WAUS.test)
ibpred <- prediction(ibpred$prob[,2], WAUS.test$RainTomorrow)
ib.cv.perf <- performance(ibpred, "tpr", "fpr")
auc_ib.cv <- performance(ibpred, measure = "auc")
print(as.numeric(auc_ib.cv@y.values))
# 4. Comparison with original Bagging model
dt <- data.frame(c(ib.acc, ib.cv.acc), c(as.numeric(auc_ib@y.values), as.numeric(auc_ib.cv@y.values)))
colnames(dt) <- c("Acc", "AUC")
rownames(dt) <- c("mfinal = 10", "mfinal = 100")
dt
plot(ibperf, col = "blue")
plot(ib.cv.perf, col = "red", add = TRUE)
abline(0,1)
legend('bottomright', c("mfinal = 10", "mfinal = 100"), lty=1, col=c('blue', 'red'))
getwd
getwd()
setwd("Uni/Year 3 Sem 1/FIT3152/Assignments/Assignment 2/Forecasting Weather in Australia")
#------------------------------------------------------------------------------------------------------
# Script-file:   WAUS2019.R
# Author:        Chun Wen Khoo
#	Project:       Rain in Australia
# Purpose:  	   Predict rain tomorrow in Australia
#------------------------------------------------------------------------------------------------------
# Clean working environment
rm(list = ls())
# Set number of significant digits to 4
options(digits = 4)
#------------------------------------------------------------------------------------------------------
# Loading data, sample 2000 rows and 10 locations
#------------------------------------------------------------------------------------------------------
WAUS <- read.csv("WAUS2019.csv")
L <- as.data.frame(c(1:49))
set.seed(12345678)
L <- L[sample(nrow(L), 10, replace = FALSE),] # sample 10 locations
WAUS <- WAUS[(WAUS$Location %in% L),]
WAUS <- WAUS[sample(nrow(WAUS), 2000, replace = FALSE),] # sample 2000 rows
#------------------------------------------------------------------------------------------------------
# Data pre-processing
#------------------------------------------------------------------------------------------------------
# Plot of proportion of rainy days to fine days
library(reshape2)
library(ggplot2)
w <- WAUS[c('Location', 'RainToday', 'RainTomorrow')]
w <- w[!(is.na(w$RainToday)) & !(is.na(w$RainTomorrow)),]
dfm <- melt(w, id.vars = 'Location')
View(dfm)
g <- ggplot(dfm, aes(x = variable, fill = value)) + geom_bar(position = "dodge") + ggtitle("Proportion of rainy days and fine days")
g <- g + labs(y = "Number of days")
g
# Proportion of actual rainy days to actual fine days
cur <- WAUS[!(is.na(WAUS$RainToday)),]
rain <- cur[cur$RainToday == "Yes",]
fine <- cur[cur$RainToday == "No",]
nrow(fine)/nrow(rain)
# Proportion of predicted rainy days to predicted fine days
cur <- WAUS[!(is.na(WAUS$RainTomorrow)),]
rain <- cur[cur$RainTomorrow == "Yes",]
fine <- cur[cur$RainTomorrow == "No",]
nrow(fine)/nrow(rain)
# Summary of each numeric predictor
library(pastecs)
stat.desc(WAUS[c(5:9,11,14:23)], norm = FALSE)
#------------------------------------------------------------------------------------------------------
# Attributes to omit from analysis:
# Day, month, and year :- would not contribute to the prediction of weather of the next day.
# Location :- Only want to predict if it will rain in Australia, hence not location-specific.
# Evaporation, sunshine, Cloud9am, Cloud3pm :- ignore columns with less data (threshold ~ 60%)
colSums(!is.na(WAUS))/2000*100
WAUS <- WAUS[,c(5:7,10:19,22:25)]
# Remove rows that contain NAs
WAUS <- na.omit(WAUS)
# Split to training and testing sets
set.seed(12345678)
train.row = sample(1:nrow(WAUS), 0.7*nrow(WAUS))
WAUS.train = WAUS[train.row,]
WAUS.test = WAUS[-train.row,]
#------------------------------------------------------------------------------------------------------
# Libraries to be used
library(tree)
library(e1071)
library(adabag)
library(randomForest)
library(ROCR)
#------------------------------------------------------------------------------------------------------
# Classification models
#------------------------------------------------------------------------------------------------------
# Decision tree
# 1. Fitting the model
set.seed(12345678)
WAUS.tree <- tree(RainTomorrow~., data = WAUS.train)
plot(WAUS.tree)
text(WAUS.tree, pretty = 0)
# 2. Predict using test set, produce confusion matrix
tpred <- predict(WAUS.tree, WAUS.test, type = "class")
tpred <- table(actual = WAUS.test$RainTomorrow, predicted = tpred)
t.acc <- (tpred[1] + tpred[4])/sum(tpred)
t.acc
# 3. Calculate confidence of predicting 'Yes' for each case and plot ROC curve for each classifier
tpred.r <- predict(WAUS.tree, WAUS.test, type = "vector")
tpred <- prediction(tpred.r[,2], WAUS.test$RainTomorrow)
tperf <- performance(tpred, "tpr", "fpr")
auc_tree <- performance(tpred, measure = "auc")
#------------------------------------------------------------------------------------------------------
# Naive Bayes
# 1. Fitting the model
set.seed(12345678)
WAUS.nbmodel <- naiveBayes(RainTomorrow~., data = WAUS.train)
# 2. Predict using test set, produce confusion matrix
nbpred <- predict(WAUS.nbmodel, WAUS.test)
nbpred <- table(actual = WAUS.test$RainTomorrow, predicted = nbpred)
nb.acc <- (nbpred[1] + nbpred[4])/sum(nbpred)
nb.acc
# 3. Calculate confidence of predicting 'Yes' for each case and plot ROC curve for each classifier
nbpred.r <- predict(WAUS.nbmodel, WAUS.test, type = "raw")
nbpred <- prediction(nbpred.r[,2], WAUS.test$RainTomorrow)
nbperf <- performance(nbpred, "tpr", "fpr")
auc_nb <- performance(nbpred, measure = "auc")
#------------------------------------------------------------------------------------------------------
# Bagging
# 1. Fitting the model
set.seed(12345678)
WAUS.bag <- bagging(RainTomorrow~., data = WAUS.train, mfinal = 10)
# 2. Predict using test set, produce confusion matrix
ibpred <- predict.bagging(WAUS.bag, newdata = WAUS.test)
ibpred$confusion
ib.acc <- (ibpred$confusion[1] + ibpred$confusion[4])/sum(ibpred$confusion)
ib.acc
# 3. Calculate confidence of predicting 'Yes' for each case and plot ROC curve for each classifier
ibpred <- predict.bagging(WAUS.bag, newdata = WAUS.test)
ibpred <- prediction(ibpred$prob[,2], WAUS.test$RainTomorrow)
ibperf <- performance(ibpred, "tpr", "fpr")
auc_ib <- performance(ibpred, measure = "auc")
#------------------------------------------------------------------------------------------------------
# Boosting
# 1. Fitting the model
set.seed(12345678)
WAUS.boost <- boosting(RainTomorrow~., data = WAUS.train, mfinal = 10)
# 2. Predict using test set, produce confusion matrix
ibopred <- predict.boosting(WAUS.boost, newdata = WAUS.test)
ibopred <- table(observed = WAUS.test$RainTomorrow, predicted = ibopred$class)
ibo.acc <- (ibopred[1]+ibopred[4])/sum(ibopred)
ibo.acc
# 3. Calculate confidence of predicting 'Yes' for each case and plot ROC curve for each classifier
ibopred.r <- predict(WAUS.boost, WAUS.test, type = "raw")
ibopred <- prediction(ibopred.r$prob[,2], WAUS.test$RainTomorrow)
iboperf <- performance(ibopred, "tpr", "fpr")
auc_ibo <- performance(ibopred, measure = "auc")
#------------------------------------------------------------------------------------------------------
# Random forest
# 1. Fitting the model
set.seed(12345678)
WAUS.rf <- randomForest(RainTomorrow~., data = WAUS.train, importance = TRUE)
# 2. Predict using test set, produce confusion matrix
rfpred <- predict(WAUS.rf, WAUS.test)
rfpred <- table(observed = WAUS.test$RainTomorrow, predicted = rfpred)
rf.acc <- (rfpred[1] + rfpred[4])/sum(rfpred)
rf.acc
# 3. Calculate confidence of predicting 'Yes' for each case and plot ROC curve for each classifier
rfpred.r <- predict(WAUS.rf, WAUS.test, type = "prob")
rfpred <- prediction(rfpred.r[,2], WAUS.test$RainTomorrow)
rfperf <- performance(rfpred, "tpr", "fpr")
auc_rf <- performance(rfpred, measure = "auc")
#------------------------------------------------------------------------------------------------------
# ROC curve of each classifier
plot(tperf, col = "brown")
plot(nbperf, add = TRUE, col = "blue")
plot(ibperf, add = TRUE, col = "black")
plot(iboperf, add = TRUE, col = "red")
plot(rfperf, add = TRUE, col = "green")
abline(0,1)
title("ROC of classifiers")
legend('bottomright',
c("Decision Tree", "Naive Bayes", "Bagging", "Boosting", "Random Forest"),
lty=1, col=c('brown', 'blue', 'black',' red', 'green'))
# Accuracy and AUC of each classifier
classifier <- c("Decision Tree", "Naive Bayes","Bagging", "Boosting", "Random Forest")
acc <- c(t.acc, nb.acc, ib.acc, ibo.acc, rf.acc)
auc <- c(as.numeric(auc_tree@y.values), as.numeric(auc_nb@y.values),
as.numeric(auc_ib@y.values), as.numeric(auc_ibo@y.values), as.numeric(auc_rf@y.values))
class_acc_auc <- data.frame(classifier, acc, auc)
class_acc_auc
#------------------------------------------------------------------------------------------------------
# Which variables are most important in predicting weather, and which variables can be omitted?
# Important predictors from decision tree
summary(WAUS.tree)
# Important predictors from bagging
WAUS.bag$importance
# Important predictors from boosting
WAUS.boost$importance
# Important predictors from random forest
importance(WAUS.rf)
#------------------------------------------------------------------------------------------------------
# Experiment with classifier settings (for at least of one model). Show improved ROC, AUC, or other accuracy measures.
# Report parameter settings and assumptions made
#------------------------------------------------------------------------------------------------------
# CV and pruning with decision tree
# 1. Fitting the model
set.seed(12345678)
WAUS.tree.cv <- cv.tree(WAUS.tree, FUN = prune.misclass)
print(WAUS.tree.cv)
WAUS.prune <- prune.misclass(WAUS.tree, best = 5)
plot(WAUS.prune)
text(WAUS.prune, pretty = 0)
# 2. Predict using test set
prune.tpred <- predict(WAUS.prune, WAUS.test, type = "class")
prune.tpred <- table(actual = WAUS.test$RainTomorrow, predicted = prune.tpred)
prune.t.acc <- (prune.tpred[1] + prune.tpred[4])/sum(prune.tpred)
prune.t.acc
prune.tpred.r <- predict(WAUS.prune, WAUS.test, type = "vector")
prune.tpred <- prediction(prune.tpred.r[,2], WAUS.test$RainTomorrow)
prune.tperf <- performance(prune.tpred, "tpr", "fpr")
auc_ptree <- performance(prune.tpred, measure = "auc")
# 3. Comparison with original decision tree
plot(prune.tperf, col = "blue")
plot(tperf, col = "red", add = TRUE)
abline(0,1)
legend('bottomright', c("Original", "CV with pruning"), lty=1, col=c('blue', 'red'))
print(as.numeric(auc_ptree@y.values))
dt <- data.frame(c(t.acc, prune.t.acc), c(as.numeric(auc_tree@y.values), as.numeric(auc_ptree@y.values)))
colnames(dt) <- c("Acc", "AUC")
rownames(dt) <- c("Original", "CV with pruning")
dt
#------------------------------------------------------------------------------------------------------
# Bagging with mfinal = 100
# 1. Fitting the model
set.seed(12345678)
WAUS.bag <- bagging(RainTomorrow~., data = WAUS.train, mfinal = 100)
# 2. Predict using test set, produce confusion matrix
ibpred <- predict.bagging(WAUS.bag, newdata = WAUS.test)
ibpred$confusion
ib.cv.acc <- (ibpred$confusion[1] + ibpred$confusion[4])/sum(ibpred$confusion)
ib.cv.acc
# 3. Calculate confidence of predicting 'Yes' for each case and plot ROC curve for each classifier
ibpred <- predict.bagging(WAUS.bag, newdata = WAUS.test)
ibpred <- prediction(ibpred$prob[,2], WAUS.test$RainTomorrow)
ib.cv.perf <- performance(ibpred, "tpr", "fpr")
auc_ib.cv <- performance(ibpred, measure = "auc")
print(as.numeric(auc_ib.cv@y.values))
# 4. Comparison with original Bagging model
dt <- data.frame(c(ib.acc, ib.cv.acc), c(as.numeric(auc_ib@y.values), as.numeric(auc_ib.cv@y.values)))
colnames(dt) <- c("Acc", "AUC")
rownames(dt) <- c("mfinal = 10", "mfinal = 100")
dt
plot(ibperf, col = "blue")
plot(ib.cv.perf, col = "red", add = TRUE)
abline(0,1)
legend('bottomright', c("mfinal = 10", "mfinal = 100"), lty=1, col=c('blue', 'red'))
plot(ibperf, col = "blue", title = 'Comparison of Bagging method with varied final number of trees')
plot(ib.cv.perf, col = "red", add = TRUE)
abline(0,1)
legend('bottomright', c("mfinal = 10", "mfinal = 100"), lty=1, col=c('blue', 'red'))
title('Comparison of Bagging method with varied final number of trees')
title('Comparison of Bagging method, varying final number of trees')
plot(ibperf, col = "blue")
plot(ib.cv.perf, col = "red", add = TRUE)
abline(0,1)
legend('bottomright', c("mfinal = 10", "mfinal = 100"), lty=1, col=c('blue', 'red'))
title('Comparison of Bagging method, varying final number of trees')
plot(prune.tperf, col = "blue")
plot(tperf, col = "red", add = TRUE)
abline(0,1)
legend('bottomright', c("Original", "CV with pruning"), lty=1, col=c('blue', 'red'))
title("Comparison of Decision Tree method")
print(as.numeric(auc_ptree@y.values))
# 3. Comparison with original decision tree
dt <- data.frame(c(t.acc, prune.t.acc), c(as.numeric(auc_tree@y.values), as.numeric(auc_ptree@y.values)))
colnames(dt) <- c("Acc", "AUC")
rownames(dt) <- c("Original", "CV with pruning")
dt
